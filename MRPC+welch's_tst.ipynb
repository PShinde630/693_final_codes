{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdqArUNDOaMr",
        "outputId": "97fbe698-cc8f-4b51-d2bb-cb0a6471281b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MRPC dataset...\n",
            "Computing similarities...\n",
            "\n",
            "Model Evaluation on MRPC (1000 pairs):\n",
            "TF-IDF ➤ Accuracy: 0.4299 | F1 Score: 0.2856\n",
            "Word2Vec ➤ Accuracy: 0.6903 | F1 Score: 0.8110\n",
            "BERT ➤ Accuracy: 0.7246 | F1 Score: 0.7935\n",
            "\n",
            "Welch’s t-test between models (similarity scores):\n",
            "TF-IDF vs Word2Vec ➤ t = -111.1915, p = 0.0000\n",
            "→ Statistically significant difference ✅\n",
            "\n",
            "Word2Vec vs BERT ➤ t = 38.9491, p = 0.0000\n",
            "→ Statistically significant difference ✅\n",
            "\n",
            "TF-IDF vs BERT ➤ t = -65.3459, p = 0.0000\n",
            "→ Statistically significant difference ✅\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install numpy==1.25.2\n",
        "!pip install datasets\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from gensim.downloader import load as gensim_load\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import ttest_ind\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# ---- Load MRPC Dataset ----\n",
        "print(\"Loading MRPC dataset...\")\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "mrpc = dataset[\"train\"]\n",
        "sentences1 = mrpc[\"sentence1\"]\n",
        "sentences2 = mrpc[\"sentence2\"]\n",
        "labels = mrpc[\"label\"]\n",
        "\n",
        "# ---- TF-IDF + Cosine Similarity ----\n",
        "def compute_tfidf_similarity(sentences1, sentences2):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "    all_sentences = sentences1 + sentences2\n",
        "    vectorizer.fit(all_sentences)\n",
        "    s1 = vectorizer.transform(sentences1)\n",
        "    s2 = vectorizer.transform(sentences2)\n",
        "    return [cosine_similarity(a, b)[0][0] for a, b in zip(s1, s2)]\n",
        "\n",
        "# ---- Word2Vec + Cosine Similarity ----\n",
        "def compute_word2vec_similarity(sentences1, sentences2):\n",
        "    w2v_model = gensim_load('word2vec-google-news-300')\n",
        "    def sentence_vector(sentence):\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        vectors = [w2v_model[w] for w in tokens if w in w2v_model]\n",
        "        return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
        "    s1 = [sentence_vector(sent) for sent in sentences1]\n",
        "    s2 = [sentence_vector(sent) for sent in sentences2]\n",
        "    return [cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0] for a, b in zip(s1, s2)]\n",
        "\n",
        "# ---- BERT + Cosine Similarity ----\n",
        "def compute_bert_similarity(sentences1, sentences2):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "    s1 = model.encode(sentences1, convert_to_numpy=True)\n",
        "    s2 = model.encode(sentences2, convert_to_numpy=True)\n",
        "    return [cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0] for a, b in zip(s1, s2)]\n",
        "\n",
        "# ---- Similarity to Binary Predictions ----\n",
        "def to_predictions(similarities, threshold=0.75):\n",
        "    return [1 if s >= threshold else 0 for s in similarities]\n",
        "\n",
        "# ---- Classification Evaluation ----\n",
        "def evaluate_classification(name, preds, labels):\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    print(f\"{name} ➤ Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ---- Welch’s t-test ----\n",
        "def run_welchs_test(scores_a, scores_b, label_a=\"Model A\", label_b=\"Model B\"):\n",
        "    t_stat, p_val = ttest_ind(scores_a, scores_b, equal_var=False)\n",
        "    print(f\"{label_a} vs {label_b} ➤ t = {t_stat:.4f}, p = {p_val:.4f}\")\n",
        "    if p_val < 0.05:\n",
        "        print(\"→ Statistically significant difference ✅\\n\")\n",
        "    else:\n",
        "        print(\"→ No statistically significant difference ❌\\n\")\n",
        "\n",
        "# ---- Run All ----\n",
        "print(\"Computing similarities...\")\n",
        "\n",
        "# Compute Similarities\n",
        "tfidf_sim = compute_tfidf_similarity(sentences1, sentences2)\n",
        "word2vec_sim = compute_word2vec_similarity(sentences1, sentences2)\n",
        "bert_sim = compute_bert_similarity(sentences1, sentences2)\n",
        "\n",
        "# Binary predictions\n",
        "tfidf_preds = to_predictions(tfidf_sim)\n",
        "word2vec_preds = to_predictions(word2vec_sim)\n",
        "bert_preds = to_predictions(bert_sim)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nModel Evaluation on MRPC (1000 pairs):\")\n",
        "evaluate_classification(\"TF-IDF\", tfidf_preds, labels)\n",
        "evaluate_classification(\"Word2Vec\", word2vec_preds, labels)\n",
        "evaluate_classification(\"BERT\", bert_preds, labels)\n",
        "\n",
        "# Welch’s t-test\n",
        "print(\"\\nWelch’s t-test between models (similarity scores):\")\n",
        "run_welchs_test(tfidf_sim, word2vec_sim, \"TF-IDF\", \"Word2Vec\")\n",
        "run_welchs_test(word2vec_sim, bert_sim, \"Word2Vec\", \"BERT\")\n",
        "run_welchs_test(tfidf_sim, bert_sim, \"TF-IDF\", \"BERT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Compute Mean and Standard Deviation ----\n",
        "def compute_stats(similarities, name):\n",
        "    mean_val = np.mean(similarities)\n",
        "    sd_val = np.std(similarities)\n",
        "    median_val = np.median(similarities)\n",
        "    print(f\"{name} ➤ Mean: {mean_val:.4f} | SD: {sd_val:.4f} | Median: {median_val:.4f}\")\n",
        "    return mean_val, sd_val, median_val\n",
        "\n",
        "# ---- Print Mean, SD, Median for All Models ----\n",
        "print(\"\\nSimilarity Score Statistics:\")\n",
        "tfidf_mean, tfidf_sd, tfidf_median = compute_stats(tfidf_sim, \"TF-IDF\")\n",
        "word2vec_mean, word2vec_sd, word2vec_median = compute_stats(word2vec_sim, \"Word2Vec\")\n",
        "bert_mean, bert_sd, bert_median = compute_stats(bert_sim, \"BERT\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFVXNKxyiUgo",
        "outputId": "7fa1e924-07d3-4d5f-dff4-1baf8a0db1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similarity Score Statistics:\n",
            "TF-IDF ➤ Mean: 0.5258 | SD: 0.1872 | Median: 0.5297\n",
            "Word2Vec ➤ Mean: 0.8909 | SD: 0.0670 | Median: 0.9035\n",
            "BERT ➤ Mean: 0.7850 | SD: 0.1504 | Median: 0.8168\n"
          ]
        }
      ]
    }
  ]
}